{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353dc492-99b2-49e2-9c9e-07513613e39b",
   "metadata": {},
   "source": [
    "# Advanced Query Techniques\n",
    "\n",
    "Sometimes data analysis requires advanced SQL techniques that go beyond table joins or basic `SELECT` queries. We'll learn techniques including writing queries that use the results of other queries as inputs & reclassifying numerical values into categories, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03072e-0dec-4ae4-a3d2-f8efe9d3ddcd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4fc8e-7577-4367-938e-2fbfb361445a",
   "metadata": {},
   "source": [
    "# Using Subqueries\n",
    "\n",
    "A *subquery* is a query nested inside another query. Typically, it performs a calculation or a logical test or generates rows to be passed into the main outer query. Subqueries are part of standard ANSI SQL, & the syntax is not unusual: we enclose a query in parentheses. For example, we can write a subquery that returns multiple rows & treat those results as a table in the `FROM` clause of the main outer query. Or we can create a *scalar subquery* that returns a single value & use it as part of an *expression* to filter rows via `WHERE`, `IN`, & `HAVING` clauses. A *correlated subquery* is one that depends on a value or table name from the outer query to execute. Conversely, an *uncorrelated subqery* has no reference to objects in the main query.\n",
    "\n",
    "## Filtering with Subqueries in a WHERE Clause\n",
    "\n",
    "A `WHERE` clause lets us filter query results based on criteria we provide, using an expression such as `WHERE quantity > 1000`. But this requires that we already know the value to use for comparison. What if we don't? That's one way a subquery comes in handy: it lets us write a query that generates one or more values to use as part of an expression in a `WHERE` clause.\n",
    "\n",
    "### Generating Values for a Query Expression\n",
    "\n",
    "Say you want to write a query to show which US counties are at or above the 90th percentile, or top 10 percent, for population. Rather than writing two separate queries -- one to calculate the 90th percentile & another to find counties with populations at or higher -- you can do both at once using a subquery as part of a `WHERE` clause.\n",
    "\n",
    "```\n",
    "SELECT county_name,\n",
    "       state_name,\n",
    "       pop_est_2019\n",
    "FROM us_counties_pop_est_2019\n",
    "WHERE pop_est_2019 >= (\n",
    "    SELECT percentile_cont(0.9) WITHIN GROUP (\n",
    "               ORDER BY pop_est_2019)\n",
    "    FROM us_counties_pop_est_2019\n",
    ")\n",
    "ORDER BY pop_est_2019 DESC;\n",
    "```\n",
    "\n",
    "The `WHERE` clause, which filters by the total population column `pop_est_2019` doesn't include a value as it normally would. Instead, after the `>=` comparison operators, we provide a subquery in parentheses. This subquery uses the `percentile_cont()` function to generate one value: the 90th percentile cutoff point in the `pop_est_2019` column.\n",
    "\n",
    "This is an example of an uncorrelated subquery. It does not depend on any values in the outer query, & it will be executed just once to generate the requested value. If we run the subquery portion only, it will execute with a result of `213707.3`. But because the subquery result is passed directly to the outer query's `WHERE` clause, you won't see that number when the entire query is ran.\n",
    "\n",
    "The entire query should return 315 rows, or about 10 percent of the 3,142 rows in `us_counties_pop_est_2019`.\n",
    "\n",
    "<img src = \"Using a Subquery in a WHERE Clause.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "The result includes all counties with a population greater than or equal to `213707.3`, the value the subquery generated.\n",
    "\n",
    "### Using a Subquery to Identify Rows to Delete\n",
    "\n",
    "We can use the same subquery in a `DELETE` statement to specify what to remove from a table. We'll make a copy of the census table, then delete everything from that backup exect the 315 counties in the top 10 percent of the population.\n",
    "\n",
    "```\n",
    "CREATE TABLE us_counties_2019_top10 AS\n",
    "(SELECT * FROM us_counties_pop_est_2019);\n",
    "\n",
    "DELETE FROM us_counties_2019_top10\n",
    "WHERE pop_est_2019 < (\n",
    "    SELECT percentile_cont(0.9) WITHIN GROUP (\n",
    "               ORDER BY pop_est_2019)\n",
    "    FROM us_counties_2019_top10\n",
    ");\n",
    "\n",
    "SELECT count(*)\n",
    "FROM us_counties_2019_top10;\n",
    "```\n",
    "\n",
    "The result should be 315 rows, which is the original 3,142 minus the 2,827 below the value identified by the subquery.\n",
    "\n",
    "<img src = \"Using a Subquery in a WHERE Clause with DELETE.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "## Creating Derived Tables with Subqueries\n",
    "\n",
    "If your subquery returns rows & columns, as opposed to just a single value like in the example before, you can place it in a `FROM` clause to create a new table known as a *derived table* that you can query or join with other tables, just as you would a regular table. It's another example of an uncorrelated subquery.\n",
    "\n",
    "Let's entertain a simple example. Let's compare the average & median population of US counties, as well as the difference between them, to highlight the skewness of the population distribution. We need to calculate the average & the median & then subtract the two. We can do both operations in one fell swoop with a subquery in the `FROM` clause.\n",
    "\n",
    "```\n",
    "SELECT round(calcs.average, 0) AS average,\n",
    "       calcs.median,\n",
    "       round(calcs.average - calcs.median, 0)\n",
    "           AS median_avg_diff\n",
    "FROM (\n",
    "    SELECT avg(pop_est_2019) AS average,\n",
    "           percentile_cont(0.5) WITHIN GROUP (\n",
    "               ORDER BY pop_est_2019)::numeric\n",
    "               AS median\n",
    ") AS calcs;\n",
    "```\n",
    "\n",
    "The subquery that produces the derived table is straightforward. We use the `avg()` & `percentile_cont()` functions to find the average & median of the census table's `pop_est_2019` column &name each column with an alias. Then we name the derived table `calcs` so we can reference it in the main query.\n",
    "\n",
    "In the main query, we subtract the `median` from the `average`, both of which are returned by the subquery. The result is rounded & labeled with the alias `median_avg_diff`. The result should be the following:\n",
    "\n",
    "<img src = \"Subquery as a Derived Table in a FROM Clause.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "The difference between the median & average, 78,742, is nearly three times the size of the median. That indicates that we have some high-population counties inflating the average.\n",
    "\n",
    "## Joining Derived Tables\n",
    "\n",
    "Joining multiple derived tables lets you perform several preprocessing steps before final calculations in a main query. For example, in previous lessons, we calculated the rate of tourism-related businesses per 1,000 population in each county. Let's say we want to do that at the state level. Before we can calculate that rate, we need to know the number of tourism businesses in each state & the population of each state. The below code shows how to write subqueries for both tasks & join them to calculate the overall rate.\n",
    "\n",
    "```\n",
    "SELECT census.state_name AS st,\n",
    "       census.pop_est_2018,\n",
    "       est.establishment_count,\n",
    "       round((est.establishment_count /\n",
    "           census.pop_est_2018::numeric) * 1000, 1)\n",
    "           AS estabs_per_thousand\n",
    "FROM (\n",
    "    SELECT st,\n",
    "           sum(establishments) AS establishment_count\n",
    "    FROM cbp_naics_72_establishments\n",
    "    GROUP BY st\n",
    "    ) AS est\n",
    "JOIN (\n",
    "    SELECT state_name,\n",
    "           sum(pop_est_2018) AS pop_est_2018\n",
    "    FROM us_counties_pop_est_2019\n",
    "    GROUP BY state_name\n",
    "    ) AS census\n",
    "ON est.st = census.state_name\n",
    "ORDER BY estabs_per_thousand DESC;\n",
    "```\n",
    "\n",
    "The math & syntax in the outer query for finding `estabs_per_thousand` should be familiar. We divide the number of establishments by the population & then multiple that quotient by a thousand. For the inputs, we use the values generated from two derived tables.\n",
    "\n",
    "The first finds the number of establishments in each state using the `sum()` aggregate function. We give this derived table the alias `est` for reference in the main part of the query. We second finds the 2018 estimated population by state by using `sum()` on the `pop_est_2018` column. We alias this derived table as `census`.\n",
    "\n",
    "Next, we join the derived tables by linking the `st` column in `est` to the `state_name` column in `census`. We then list the results in descending order based on the rate. Here is the result\n",
    "\n",
    "<img src = \"Joining Two Derived Tables.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "At the top is Washington DC, unsurprising given the tourist activity generated by museums, monuments, & other attractions in the nation's capital. Montana may seem like a surprise in second place, but it's a low population state with majour tourist destinations including Glacier & Yellowstone national parts. Mississippi & Kentucky are among those states with the fewest tourism-related businesses per 1,000 population.\n",
    "\n",
    "## Generating Columns with Subqueries\n",
    "\n",
    "You can also place a subquery in the column list after `SELECT` to generate a value for that column in the query result. The subquery must generate only a single row. For example, we can select the geography & population information from `us_counties_pop_est_2019` & then adds the median of all counties to each row in the new column `us_median`.\n",
    "\n",
    "```\n",
    "SELECT county_name,\n",
    "       state_name AS st,\n",
    "       pop_est_2019,\n",
    "       (SELECT percentile_cont(0.5) WITHIN GROUP (\n",
    "            ORDER BY pop_est_2019)\n",
    "        FROM us_counties_pop_est_2019) AS us_median\n",
    "FROM us_counties_pop_est_2019;\n",
    "```\n",
    "\n",
    "The result set should look like this:\n",
    "\n",
    "<img src = \"Adding a Subquery to a Column List.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "On its own, that repeating `us_median` value isn't very helpful. It would be more interesting to generate values that indicate how much each county's population deviates from the median value. Let's look at how we can use the same subquery technique to do that. We'll build on the previous query by substituting a subquery after `SELECT` that calculates the difference between the population & the median for each county.\n",
    "\n",
    "```\n",
    "SELECT county_name,\n",
    "       state_name AS st,\n",
    "       pop_est_2019,\n",
    "       pop_est_2019 - (SELECT percentile_cont(0.5)\n",
    "           WITHIN GROUP (ORDER BY pop_est_2019)\n",
    "           FROM us_counties_pop_est_2019)\n",
    "           AS diff_from_median\n",
    "FROM us_counties_pop_est_2019\n",
    "WHERE (pop_est_2019 - (SELECT percentile_cont(0.5)\n",
    "    WITHIN GROUP (ORDER BY pop_est_2019)\n",
    "    FROM us_counties_pop_est_2019))\n",
    "    BETWEEN -1000 AND 1000;\n",
    "```\n",
    "\n",
    "The subquery is now part of a calculation that subtracts the subquery's result from `pop_est_2019`, the total population, giving the column an alias of `diff_from_median`. To make this query even more useful, we can filter results to show counties whose population is close to the median. To do this, we repeat the calculation with the subquery in the `WHERE` clause & filter results using the `BETWEEN -1000 AND 1000` expression.\n",
    "\n",
    "The outcome should reveal 78 counties.\n",
    "\n",
    "<img src = \"Using a Subquery in a Calculation.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "Bear in mind that subqueries can add to overall query execution time. We removed the subquery that displays the column `us_median` to avoid repeating the subquery another time. With our data set, the impact is minimal, but if we were working with millions of rows, winnowing some unneeded subqueries might provided a significant speed boost.\n",
    "\n",
    "## Understanding Subquery Expressions\n",
    "\n",
    "We can also use subqueries to filter rows by evaluating whether a condition evaluates to `true` or `false`. For this, we can use *subquery expressions*, which are a combination of a keyword with a subquery & are generally used in `WHERE` clauses to filter rows based on the existence of values in another table.\n",
    "\n",
    "We'll examine the syntax for two subquery expressions that tend to be used most often: `IN` & `EXISTS`. The below code will create a small table called `retirees` that we'll query along with the `employees `table. We'll imagine that we've received this data from a vendor listing people who've applied for retirement benefits.\n",
    "\n",
    "```\n",
    "CREATE TABLE retirees (\n",
    "    id int,\n",
    "    first_name text,\n",
    "    last_name text\n",
    ");\n",
    "\n",
    "INSERT INTO retirees\n",
    "VALUES (2, 'Janet', 'King'),\n",
    "       (4, 'Michael', 'Taylor');\n",
    "```\n",
    "\n",
    "### Generating Values for the IN Operator\n",
    "\n",
    "The subquery expression `IN (subquery)` works like the `IN` operator, except we employ a subquery to provide the list of values to check against rather than manually entering one. In the below query, we use an uncorrelated subquery, which will be executed one time, to generate `id` values from the `retirees` table. The values it returns become the list for the `IN` operator in the `WHERE` clause. This lets us find employees who are also present in the table of retirees.\n",
    "\n",
    "```\n",
    "SELECT first_name, last_name\n",
    "FROM employees\n",
    "WHERE emp_id IN (SELECT id FROM retirees)\n",
    "ORDER BY emp_id;\n",
    "```\n",
    "\n",
    "The output shows the two people in `employees` whose `emp_id` have a matching `id` in the `retirees` table:\n",
    "\n",
    "<img src = \"Generating Values For The IN Operator.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "### Checking Whether Values Exist\n",
    "\n",
    "The subquery expression `EXISTS (subquery)` returns a value of `true` if the subquery in parentheses returns at least one row. If it returns no rows, `EXISTS` evaluates to `false`.\n",
    "\n",
    "The `EXISTS` subquery expression below shows an example of a correlated subquery -- it includes an expression in its `WHERE` caluse that requires data from the outer query. Also, because the subquery is correlated, it will execute once for each row returned by the outer query, each time checking whether there's an `id` in `retirees` that matches `emp_id` in `employees`. If there is a match, the `EXISTS` expression returns `true`.\n",
    "\n",
    "```\n",
    "SELECT first_name, last_name\n",
    "FROM employees\n",
    "WHERE EXISTS (\n",
    "    SELECT id,\n",
    "    FROM retirees\n",
    "    WHERE id = employees.emp_id);\n",
    "```\n",
    "\n",
    "When you run the query, it should return the same result as the query above it. Using this approach is particularly helpful if you need to join on more than one column, which you can't do with the `IN` expression. You also can add the `NOT` keyword with `EXISTS` to perform the opposite function & find rows in the employees table with no corresponding record in `retirees`.\n",
    "\n",
    "```\n",
    "SELECT first_name, last_name\n",
    "FROM employees\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT id\n",
    "    FROM retirees\n",
    "    WHERE id = employees.emp_id);\n",
    "```\n",
    "\n",
    "That should produce these results:\n",
    "\n",
    "<img src = \"Using a Correlated Subquery with WHERE NOT EXISTS.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "The technique of using `NOT` with `EXISTS` is helpful for finding missing values or assessing whether a dataset is complete.\n",
    "\n",
    "## Using Subqueries with LATERAL\n",
    "\n",
    "Placing a keyword `LATERAL` before subqueries in a `FROM` clause adds several bits of functionality that help simplify otherwise complicated queries.\n",
    "\n",
    "### LATERAL with FROM\n",
    "\n",
    "First, a subquery preceded by `LATERAL` can reference tables & other subqueries that appear before it in the `FROM` clause, which can reduce redundant code by making it easy to reuse calculations.\n",
    "\n",
    "In the query below, we'll calculate the change in county population from 2018 to 2019 two ways: raw change in numbers & percent change.\n",
    "\n",
    "```\n",
    "SELECT county_name,\n",
    "       state_name,\n",
    "       pop_est_2018,\n",
    "       pop_est_2019,\n",
    "       raw_chg,\n",
    "       round(pct_chg * 100, 2) AS pct_chg\n",
    "FROM us_counties_pop_est_2019,\n",
    "    LATERAL (SELECT pop_est_2019 - pop_est_2018\n",
    "        AS raw_chg) rc,\n",
    "    LATERAL (SELECT raw_chg / pop_est_2018::numeric\n",
    "        AS pct_chg) pc\n",
    "ORDER BY pct_chg DESC;\n",
    "```\n",
    "\n",
    "In the `FROM` clause, after naming the us_counties_pop_est_2019 table, we add the first `LATERAL` subquery. In parentheses, we place a query that subtracts the 2018 population estimate from the 2019 estimate & alias the result as `raw_chg`. Because the `LATERAL` subquery can reference a table listed before it in the `FROM` clause without needing to specify its name, we can omit the `us_counties_pop_est_2019` table from the subquery. Subqueries in `FROM` must have an alias, so we label this one `rc`.\n",
    "\n",
    "The second `LATERAL` subquery calculates the percent change in population from 2018 to 2019. To find the percent change, we must know the raw change. Rather than re-calculate it, we can reference the `raw_chg` value from the previous subquery. That helps make our code shorter & easier to read.\n",
    "\n",
    "The query results should look like this:\n",
    "\n",
    "<img src = \"Using LATERAL Subqueries in the FROM Clause.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "### LATERAL with JOIN\n",
    "\n",
    "Combining `LATERAL` with `JOIN` creates functionality similar to a *for loop* in a programming language: for each row generated by the query in front of the `LATERAL` join, a subquery or function after the `LATERAL` join will be evaluated once.\n",
    "\n",
    "We'll reuse the `teachers` table & create a new table to record each time a teacher swipes a badge to unlock a lab door. Our task is to find the two most recent times a teacher accessed a lab.\n",
    "\n",
    "```\n",
    "ALTER TABLE teachers ADD CONSTRAINT id_key\n",
    "    PRIMARY KEY (id);\n",
    "\n",
    "CREATE TABLE teachers_lab_access (\n",
    "    access_id bigint PRIMARY KEY\n",
    "        GENERATED ALWAYS AS IDENTITY,\n",
    "    access_time timestamp with time zone,\n",
    "    lab_name text,\n",
    "    teacher_id bigint REFERENCES teachers (id)\n",
    ");\n",
    "\n",
    "INSERT INTO teachers_lab_access (\n",
    "    access_time, lab_name, teacher_id\n",
    ")\n",
    "VALUES ('2022-11-30 08:59:00-08', 'Science A', 2),\n",
    "       ('2022-12-01 08:58:00-08', 'Chemistry B', 2),\n",
    "       ('2022-12-21 09:01:00-08', 'Chemistry A', 2),\n",
    "       ('2022-12-02 11:01:00-08', 'Science B', 6),\n",
    "       ('2022-12-07 10:02:00-08', 'Science A', 6),\n",
    "       ('2022-12-17 16:00:00-08', 'Science B', 6);\n",
    "\n",
    "SELECT t.first_name, t.last_name, a.access_time,\n",
    "       a.lab_name\n",
    "FROM teachers AS t\n",
    "LEFT JOIN LATERAL (SELECT * FROM teachers_lab_access\n",
    "                   WHERE teacher_id = t.id\n",
    "                   ORDER BY access_time DESC\n",
    "                   LIMIT 2) AS a\n",
    "ON true\n",
    "ORDER BY t.id;\n",
    "```\n",
    "\n",
    "First, we add a primary key to the `teachers` table using `ALTER TABLE`. Next, we make a simple `teachers_lab_access` table with columns to record the lab name & access timestamp. The table has surrogate primary key `access_id` & a foreign key `teacher_id` that references `id` in `teachers`. Finally, we add six rows to the table using an `INSERT` statement.\n",
    "\n",
    "Now we're ready to query the data. In our `SELECT` statement, we join `teachers` to a subquery using `LEFT JOIN`. We add the `LATERAL` keyword, which means for each row returned from `teachers`, the subquery will execute, returning the two most recent labs accessed by that particular teacher & the times they were accessed. using `LEFT JOIN` will return all rows from `teachers` regardless of whether the subquery finds a matching teacher in `teachers_lab_accesss`.\n",
    "\n",
    "In the `WHERE` clause, the subquery references the outer query using the foreign key of `teacher_lab_access`. This `LATERAL` join syntax requires that the subquery have an alias, which here is `a`, & the value `true` in the `ON` portion of the `JOIN` clause. In this case, `true` lets us create the join without naming specific columns to join upon.\n",
    "\n",
    "The results should look like this:\n",
    "\n",
    "<img src = \"Using a Subquery with a LATERAL Join.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "The two teachers with IDs in the access table have their two most recent lab access times show. Teachers who didn't access a lab display `NULL` values; if we want to remove those from the results, we could substitue `INNER JOIN` (or just `JOIN`) for `LEFT JOIN`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19e304-665c-409a-b917-95855c1fbcd0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1a046-cd4e-45c5-a0af-190b8c388659",
   "metadata": {},
   "source": [
    "# Using Common Table Expressions\n",
    "\n",
    "The *common table expression* (CTE), a relatively recent addition to standard SQL, allows us to use one or more `SELECT` queries to predefine temporary tables that you can reference as often as needed in your main query. CTEs are informally called `WITH` queries because you define them using a `WITH .. AS` statement. The following examples show some advantages of using them, including cleaner code & less redundancy.\n",
    "\n",
    "The query below shows a simple CTE based on our census estimates data. The code determines how many counties in each state have 100,000 people or more.\n",
    "\n",
    "```\n",
    "WITH large_counties (\n",
    "    county_name, state_name, pop_est_2019\n",
    ")\n",
    "AS (SELECT county_name, state_name, pop_est_2019\n",
    "    FROM us_counties_pop_est_2019\n",
    "    WHERE pop_est_2019 >= 100000)\n",
    "SELECT state_name, count(*)\n",
    "FROM large_counties\n",
    "GROUP BY state_name\n",
    "ORDER BY count(*) DESC;\n",
    "```\n",
    "\n",
    "The `WITH ... AS` statement defines the temporary table `large_counties`. After `WITH`, we name the table & list its column names in parentheses. Unlike column definitions in a `CREATE TABLE` statement, we don't need to provide data types, because the temporary table inherits those from the subquery, which are enclosed in parentheses after `AS`. The subquery must return the same number of columns as defined in the temporary table, but the column names don't need to match. The column list is optional if you're not renaming columns.\n",
    "\n",
    "The main query counts & groups the rows in `large_counties` by `state_name` & then orders by the counts in descending order. The top six rows of the results should look like this:\n",
    "\n",
    "<img src = \"Using a Simple CTE to Count Large Counties.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "Texas, Florida, & California are among the states that had the most counties with a 2019 population of 100,000 or more.\n",
    "\n",
    "The query below uses a CTE to rewrite the join of derived tables (finding the rate of tourism-related businesses per 1,000 population in each state) into a more readable format.\n",
    "\n",
    "```\n",
    "WITH counties (st, pop_est_2018) AS (\n",
    "         SELECT state_name, sum(pop_est_2018)\n",
    "         FROM us_counties_pop_est_2019\n",
    "         GROUP BY state_name),\n",
    "     establishments (st, establishment_count) AS (\n",
    "         SELECT st, sum(establishments) AS establishment_count\n",
    "         FROM cbp_naics_72_establishments\n",
    "         GROUP BY st)\n",
    "SELECT counties.st,\n",
    "       pop_est_2018,\n",
    "       establishment_count,\n",
    "       round((establishments.establishment_count /\n",
    "           counties.pop_est_2018::numeric(10, 1)) * 1000, 1)\n",
    "           AS estabs_per_thousand\n",
    "FROM counties JOIN establishments\n",
    "ON counties.st = establishments.st\n",
    "ORDER BY estabs_per_thousand DESC;\n",
    "```\n",
    "\n",
    "Following the `WITH` keyword, we define two tables using subqueries. The first subquery, `counties` returns the 2018 population of each state. The second, `establishments`, returns the number of tourism-related businesses per state. With those tables defined, we join them on the `st` column in each table & calculate the rate per thousand. The results are identical to the joined derived tables from before, just easier to comprehend.\n",
    "\n",
    "<img src = \"Using CTEs in a Table Join.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "As another example, we can use a CTE to simplify queries that have redundant code. For example, we used a subquery with the `percentile_cont()` function in two locations to find median county population. We can write that subquery just once as a CTE.\n",
    "\n",
    "```\n",
    "WITH us_median AS (\n",
    "    SELECT percentile_cont(0.5) WITHIN GROUP (\n",
    "               ORDER BY pop_est_2019)\n",
    "               AS us_median_pop\n",
    "    FROM us_counties_pop_est_2019)\n",
    "SELECT county_name,\n",
    "       state_name AS st,\n",
    "       pop_est_2019,\n",
    "       us_median_pop,\n",
    "       pop_est_2019 - us_median_pop AS diff_from_median\n",
    "FROM us_counties_pop_est_2019 CROSS JOIN us_median\n",
    "WHERE (pop_est_2019 - us_median_pop)\n",
    "    BETWEEN -1000 AND 1000;\n",
    "```\n",
    "\n",
    "After the `WITH` keyword, we define `us_median` as the median population using `percentile_cont()`. Then, we reference the `us_median_pop` column on its own, as part of a calculated column, & in a `WHERE` clause. To make the value available to every row in the `us_counties_pop_est_2019` table during `SELECT`, we use `CROSS JOIN`.\n",
    "\n",
    "This query provides identical results to our previous similar query, but we had to write the subquery that finds the median only once. Another bonus is that you can more easily revise the query. For example, to find counties whose population is close to the 90th percentile, we need to substitute `0.9` for `0.5` as input to `percentile_cont()` in only one place.\n",
    "\n",
    "Readable code, less redundancy, & easier modifications are often-cited reasons for using CTEs. Another is the ability to add a `RECURSIVE` keyword that lets the CTE loop through query results within the CTE itself -- a task useful when dealing with data organised in a hierarchy. You can learn more about recursive query syntax via the PostgreSQL documentation at [https://www.postgresql.org/docs/current/queries-with.html](https://www.postgresql.org/docs/current/queries-with.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd78493-6d95-4699-98e4-e4cf99adfa18",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be43e6-9ea5-43dc-9a79-a89cbc19887d",
   "metadata": {},
   "source": [
    "# Performing Cross Tabulations\n",
    "\n",
    "*Cross tabulations* provides a simple way to summarise & compare variables by displaying them in a table layout, or matrix. Rows in the matrix represent one variable, columns represent another variable, & each cell where a row & column intersect holds a value, such as a count or percentage. \n",
    "\n",
    "You'll often see cross tabulations, also called *pivot_tables* or *crosstabs*, used to report summarise of survey results or to compare pairs of variables. A frequent example happens during elections when candidates' votes are tallied by geography:\n",
    "\n",
    "|candidate|ward 1|ward 2|ward 3|\n",
    "|:---|:---|:---|:---|\n",
    "|Collins|602|1799|2112|\n",
    "|Banks|599|1398|1616|\n",
    "|Rutherford|911|902|1114|\n",
    "\n",
    "In this case, the candidates' names are one variable, the wards (or city districts) are nother variable, & the cells at the intersection of the two hold the vote totals for that candidate in that ward. Let's look at how to generate cross tabulations.\n",
    "\n",
    "## Installing the crosstab() Function\n",
    "\n",
    "Standard ANSI SQL doesn't have a crosstab function, but PostgreSQL does as part of a *module* you can install easily. Modules are PostgreSQL extras that aren't part of the core application; they include functions related to security, text search, & more. You can find a list of PostgreSQL modules at [https://www.postgresql.org/docs/current/contrib.html](https://www.postgresql.org/docs/current/contrib.html).\n",
    "\n",
    "PostgreSQL's `crosstab()` function is part of the `tablefunc` module. To install `tablefunc`, execute this command in pgAdmin:\n",
    "\n",
    "```\n",
    "CREATE EXTENSION tablefunc;\n",
    "```\n",
    "\n",
    "PostgreSQL should return the message `CREATE EXTENSION` (If we're working with another database management system, check its documentation for a similar functionality.)\n",
    "\n",
    "## Tabulating Survey Results\n",
    "\n",
    "Let's say your company needs a fun employee activity so you coordinate an ice cream social at each of your three offices. The trouble is that people are particular about ice cream flavors. To choose flavors people will like in each office, you decide to conduct a survey.\n",
    "\n",
    "The CSV file *ice_cream_survey.csv* contains 200 responses to your survey. Each row includes `response_id`, `office`, & `flavor`. You'll need to count how many people chose each flavor at each office & share the results in a readable way. \n",
    "\n",
    "We'll create the `ice_cream_survey` table.\n",
    "\n",
    "```\n",
    "CREATE TABLE ice_cream_survey (\n",
    "    response_id integer PRIMARY KEY,\n",
    "    office text,\n",
    "    flavor text\n",
    ");\n",
    "\n",
    "COPY ice_cream_survey\n",
    "FROM '/YourDirectory/ice_cream_survey.csv'\n",
    "WITH (FORMAT CSV, HEADER)\n",
    "```\n",
    "\n",
    "If you want to inspect the data, you can view the first five rows.\n",
    "\n",
    "```\n",
    "SELECT *\n",
    "FROM ice_cream_survey\n",
    "ORDER BY response_id\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "The data should look like this:\n",
    "\n",
    "<img src = \"Creating & Filling the ice_cream_survey Table.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "It looks like chocolate is in the elad! But let's confirm this choice by generating a crosstab.\n",
    "\n",
    "```\n",
    "SELECT *\n",
    "FROM crosstab('SELECT office,\n",
    "                      flavor,\n",
    "                      count(*)\n",
    "               FROM ice_cream_survey\n",
    "               GROUP BY office, flavor\n",
    "               ORDER BY office',\n",
    "\n",
    "              'SELECT flavor\n",
    "               FROM ice_cream_survey\n",
    "               GROUP BY flavor\n",
    "               ORDER BY flavor')\n",
    "AS (office text,\n",
    "    chocolate bigint,\n",
    "    strawberry bigint,\n",
    "    vanilla bigint);\n",
    "```\n",
    "\n",
    "The query begins with a `SELECT *` statement that selects everything from the contents of the `crosstab()` function. We supply two queries as parameters to the `crosstab()` function; note that because these queries are parameters, we place them inside single quotes. The first query generates the data for the crosstab & has three required columns. The first column, `office`, supplies the row names for the crosstab. The second column, `flavor`, supplies the category (or column) name to be associated with the value provided in the third column. Those values will display in each cell where a row & a column intersect in the table. In this case, we want the intersecting cells to show a `count()` of each flavor selected at each office. This first query on its own creates a simple aggregated list.\n",
    "\n",
    "The second query parameter produces the category names for the columns. The `crosstab()` function requires that the second subquery returns only one column, so we use `SELECT` to retrieve `flavor` & `GROUP BY` to return that column's unique values.\n",
    "\n",
    "Then we specify the names & data types of the crosstab's output columns following the `AS` keyword. The list must match the row & column names in the order the queries generate them. For example, because the second query that supplies the category columns orders the flavors alphabetically, the output column list must as well.\n",
    "\n",
    "When we run the code, our data displays in a clean, readable crosstab:\n",
    "\n",
    "<img src = \"Generating the Ice Cream Survey Crosstab.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "It's easy to see at a glance that the Midtown office favors chocolate but has no interest in strawberry, which is represented by a `NULL` value showing that strawberry received no votes. But strawberry is the top choice Downtown, & the Uptown office is more even split among the three flavors.\n",
    "\n",
    "## Tabulating City Temperature Readings\n",
    "\n",
    "Let's create another crosstab, but this time using real data. The *temperature_readings.csv* file contains a year's worth of daily temperature readings from three observation stations around the United States: Chicago, Seattle, & Waikiki, a neighborhood on the south shore of the city of Honolulu. The data comes form the US National Oceanic & Atmospheric Administriation (NOAA).\n",
    "\n",
    "Each row in the CSV file contains four values: the station name, the date, & the day's maximum & minimum temperatures. All temperatures are in Fahrenheit. For each month in each city, we want to compare climates using the median high temperature. We'll create the `temperature_readings` table & import the CSV file.\n",
    "\n",
    "```\n",
    "CREATE TABLE temperature_readings (\n",
    "    station_name text,\n",
    "    observation_date date,\n",
    "    max_temp integer,\n",
    "    min_temp integer,\n",
    "    CONSTRAINT temp_key PRIMARY KEY (\n",
    "        station_name, observation_date)\n",
    ");\n",
    "\n",
    "COPY temperature_readings\n",
    "FROM '/YourDirectory/temperature_readings.csv'\n",
    "WITH (FORMAT CSV, HEADER);\n",
    "```\n",
    "\n",
    "The table contains the cour columns from the CSV file; we add a natural primary key using the station name & observation date. A quick count should return 1,077 rows. Now, let's see what cross tabulating the data does.\n",
    "\n",
    "```\n",
    "SELECT *\n",
    "FROM crosstab('SELECT station_name,\n",
    "                      date_part(''month'',\n",
    "                          observation_date),\n",
    "                      percentile_cont(0.5) WITHIN\n",
    "                          GROUP (ORDER BY max_temp)\n",
    "               FROM temperature_readings\n",
    "               GROUP BY station_name,\n",
    "                        date_part(''month'',\n",
    "                            observation_date)\n",
    "               ORDER BY station_name',\n",
    "              'SELECT month\n",
    "               FROM generate_series(1,12) month')\n",
    "AS (station text,\n",
    "    jan numeric(3, 0),\n",
    "    feb numeric(3, 0),\n",
    "    mar numeric(3, 0),\n",
    "    apr numeric(3, 0),\n",
    "    may numeric(3, 0),\n",
    "    jun numeric(3, 0),\n",
    "    jul numeric(3, 0),\n",
    "    aug numeric(3, 0),\n",
    "    sep numeric(3, 0),\n",
    "    oct numeric(3, 0),\n",
    "    nov numeric(3, 0),\n",
    "    dec numeric(3, 0));\n",
    "```\n",
    "\n",
    "The crosstab structure is the same as before. The first subquery inside the `crosstab()` generates the data for the crosstab, finding the median maximum temperature for each month. It supplies three required columns. The first, `station_name`, names the rows. The second column uses the `date_part()` function to extract the month from `observation_date`, which provides the crosstab columns. Then we use `percentile_cont(0.5)` to find the median of `max_temp`. We group by station name & month so we have a median `max_temp` for each month at each station.\n",
    "\n",
    "The second subquery produces the set of category names for the columns. `generate_series()` creates a list of numbers from 1 to 12 that match the month numbers `date_part()` extracts from `observation_date`.\n",
    "\n",
    "Following `AS`, we provide the names & data types for the crosstab's output columns. Each is a `numeric` type, matching the output of the percentile function. The following output is practically poetry:\n",
    "\n",
    "<img src = \"Generating the Temperature Readings Crosstab.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "We've transformed a raw set of daily readings into compact table showing the medain maximum temperature each month for each station. At a glance, we can see that the temperature in Waikiki is consistently balmy, whereas Chicago's median high temperatures vary from just above freezing to downright pleasant. Seattle falls between the two.\n",
    "\n",
    "Crosstabs do take time to set up, but viewing datasets in a matrix often makes comparisons easier than viewing the same data in a vertical list. Keep in mind that the `crosstab()` function is resource-intensive, so tread carefully when quering sets that have millions of billions of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a74a53-0d6a-41b9-9d0f-62713e9bf4f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7df6c-aa68-4dba-86ec-0be86a4937c2",
   "metadata": {},
   "source": [
    "# Reclassifying Values with CASE\n",
    "\n",
    "The ANSI Standard SQL `CASE` statement is a *conditional expression*, meaning it lets you add some \"if this, then ...\" logic to a query. You can use `CASE` in multiple ways, but for data analysis, it's handy for reclassifying values into categories. You can create categories based on ranges in your data & classify values according to those categories.\n",
    "\n",
    "The `CASE` syntax follows this pattern:\n",
    "\n",
    "```\n",
    "CASE WHEN condition THEN result\n",
    "     WHEN another_condition THEN result\n",
    "     ELSE result\n",
    "END\n",
    "```\n",
    "\n",
    "We give the `CASE` keyword & then provide at least one `WHEN condition THEN result` clause, where `condition` is any expression the database can evaluate as `true` or `false`, such as `county = 'Dutchess County'` or `date > '1955-08-09'`. If the county is `true`, the `CASE` statement returns the `result` & stops checking any further conditions. The result can be any valid data type. If the condition is `false`, the database moves on to evaluate the next condition.\n",
    "\n",
    "To evaluate more conditions, we can add optional `WHEN ... THEN` clauses. We can also provide an optional `ELSE` clause to return a result in case no condition evaluates as `true`. Without an `ELSE` clause, the statement would return a `NULL` when no conditions are `true`. The statement finishes with an `END` keyword.\n",
    "\n",
    "The query below hsows how to use the `CASE` statement to reclassify the temperature readings in descriptive groups (named according to my own bias against cold weather).\n",
    "\n",
    "```\n",
    "SELECT max_temp,\n",
    "       CASE WHEN max_temp >= 90 THEN 'Hot'\n",
    "            WHEN max_temp >= 70 AND\n",
    "                max_temp < 90 THEN 'Warm'\n",
    "            WHEN max_temp >= 50 AND\n",
    "                max_temp < 70 THEN 'Pleasant'\n",
    "            WHEN max_temp >= 30 AND\n",
    "                max_temp < 50 THEN 'Cold'\n",
    "            WHEN max_temp < 30 THEN 'Inhumane'\n",
    "       END AS temperature_group\n",
    "FROM temperature_readings\n",
    "ORDER BY station_name, observation_date;\n",
    "```\n",
    "\n",
    "We create fives ranges for the `max_temp` column in `temperature_readings`, which we define using comparison operators. The `CASE` statement evaluates each value to find whether any of the six expressions are `true`. If so, the statement outputs the appropriate text. Not that the ranges account for all possible values in the column, leaving no gaps. If none of the statemsnt is `true`, then the `ELSE` clause assigns the value to the category `No reading`.\n",
    "\n",
    "The output should look like this.\n",
    "\n",
    "<img src = \"Reclassifying Temperature Data with CASE.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "Now that we've collapsed the data into five categories, we can use these categories to compare climate among the three cities in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a2f80-35aa-4917-a6f4-cd11577bb56a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdf222-0c9e-409d-a16d-9d29c9fae98e",
   "metadata": {},
   "source": [
    "# Using CASE in a Common Table Expression\n",
    "\n",
    "The operator we performed with `CASE` on the temperature data in the previous section is a good example of a preprocessing step you could use in a CTE. Now that we've grouped the temperatures in categories, let's count the groups by city in a CTE to see how many days of the year fall into each temperature category.\n",
    "\n",
    "The query below reclassifies the daily maximum temperatures, recast to geenrate a `temps_collapsed` CTE & then uses it for an analysis.\n",
    "\n",
    "```\n",
    "WITH temps_collapsed (\n",
    "    station_name, max_temperature_group\n",
    ") AS (\n",
    "    SELECT station_name,\n",
    "           CASE WHEN max_temp >= 90 THEN 'Hot'\n",
    "                WHEN max_temp >= 70 AND\n",
    "                    max_temp < 90 THEN 'Warm'\n",
    "                WHEN max_temp >= 50 AND\n",
    "                    max_temp < 70 THEN 'Pleasant'\n",
    "                WHEN max_temp >= 30 AND\n",
    "                    max_temp < 50 THEN 'Cold'\n",
    "                WHEN max_temp < 30 THEN 'Inhumane'\n",
    "           END AS temperature_group\n",
    "    FROM temperature_readings           \n",
    ")\n",
    "SELECT station_name, max_temperature_group, count(*)\n",
    "FROM temps_collapsed\n",
    "GROUP BY station_name, max_temperature_group\n",
    "ORDER BY station_name, count(*) DESC;\n",
    "```\n",
    "\n",
    "This code reclassifies the temperatures & then counts & groups by station name to find general climate classification of each city. The `WITH` keyword defines the CTE of `temps_collapsed`, which has two columns: `station_name` & `max_temperature_group`. We then run a `SELECT` query on the CTE, performing straightforward `count(*)` & `GROUP BY` operations on both columns. The results should look like this:\n",
    "\n",
    "<img src = \"Using CASE in a CTE.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "Using this classification scheme, the amazingly consistent Waikiki weather, with `Warm` maximum temperatures 361 days of the year, confirming its appeal as a vacation destination. From a temperature standpoint, Seattle looks good too, with a nearly 300 days of `Pleasant` or `Warm` high temps (although this belies Seattle's legendary rainfall). Chicago, with 26 days of `Inhumane` max temps, is probably not for me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0053b79-6eb5-4fad-bdba-d1a79c598dd8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877aac4-dc06-4f97-a392-12151c78ad99",
   "metadata": {},
   "source": [
    "# Wrapping Up\n",
    "\n",
    "We can now add subqueries in multiple locations to provide finer control over filtering or preprocessing data before analysing it in a main query. You can also visualise data in a matrix using cross tabulations & reclassify data into groups; both techniques give us more ways to find & tell stories using our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
